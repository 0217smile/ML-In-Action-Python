{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    一）标准化(Standardizing)\n",
    "        1.1 scale----零均值单位方差\n",
    "        1.2 StandardScaler\n",
    "    二）归一化(Rescaling)\n",
    "        2.1 MinMaxScaler(最小最大值标准化)\n",
    "        2.2 MaxAbsScaler（绝对值最大标准化）\n",
    "        2.3 对稀疏数据进行标准化\n",
    "        2.4 对离群点进行标准化\n",
    "    三）正则化(Normalizing)\n",
    "        3.1  L1、L2正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一)标准化\n",
    "### 1.1 scale-----零均值单位方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# raw_data\n",
    "X = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 StandardScaler----  计算训练集的平均值和标准差，以便测试数据集使用相同的变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  2.],\n",
       "       [ 2.,  0.,  0.],\n",
       "       [ 0.,  1., -1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.        ,  0.33333333])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Root\\0-soft\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function std_ is deprecated; Attribute ``std_`` will be removed in 0.19. Use ``scale_`` instead\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.81649658,  0.81649658,  1.24721913])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#可以直接使用训练集对测试集数据进行转换\n",
    "scaler.transform([-1., 1., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二) 归一化----将数据特征缩放至某一范围(scaling features to a range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 另外一种标准化方法是将数据缩放至给定的最小值与最大值之间，通常是０与１之间，可用MinMaxScaler实现。或者将最大的绝对值缩放至单位大小，可用MaxAbsScaler实现。\n",
    "\n",
    "### 使用这种标准化方法的原因是，有时数据集的标准差非常非常小，有时数据中有很多很多零（稀疏数据）需要保存住０元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MinMaxScaler(最小最大标准化)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 公式：X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) ; \n",
    "\n",
    "#### X_scaler = X_std/ (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.        ,  1.        ],\n",
       "       [ 1.        ,  0.5       ,  0.33333333],\n",
       "       [ 0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例子：将数据缩放至[0， 1]之间\n",
    "X_train = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将上述得到的scaler参数应用到测试数据\n",
    "X_test = np.array([[-3., -1., 4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.5       ,  0.5       ,  0.33333333]),\n",
       " array([ 0.        ,  0.5       ,  0.33333333]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可以用一下方法查看scaler的参数\n",
    "min_max_scaler.scale_,min_max_scaler.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MaxAbsScaler(绝对值最大标准化)\n",
    "#### 与上述标准化方法相似，但是它通过除以最大值将训练集缩放至[-1,1]。这意味着数据已经以０为中心或者是含有非常非常多０的稀疏数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -1. ,  2. ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-3., -1., 4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  2.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三) 正则化\n",
    "###      3.1 L1、L2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1.,-1.,2.], [2.,0.,0.],[0.,1.,-1.]])\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalizer(copy=True, norm='l2')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用processing.Normalizer()类实现对训练集和测试集的拟合和转换\n",
    "normalizer = preprocessing.Normalizer().fit(X)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四)二值化\n",
    "### 4.1 特征二值化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征二值化是把数值特征转化成布尔值的过程。这个方法对符合多变量被努力分布的输入数据进行预测概率参数很有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binarizer(copy=True, threshold=0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input\n",
    "X = np.array([[1.,-1.,2.], [2.,0.,0.],[0.,1.,-1.]])\n",
    "#binary\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n",
    "binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调整阈值\n",
    "binarizer = preprocessing.Binarizer(threshold=1.1)\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注：稀疏数据输入：\n",
    "\n",
    "binarize 和 Binarizer 既接受稠密数据（dense array-like），也接受稀疏矩阵（from scipy.sparse）作为输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五) 对类别特征进行编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、One-Hot Encoding\n",
    "    One-Hot编码，又称为一位有效编码，主要是采用位状态寄存器来对个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。\n",
    "    在实际的机器学习的应用任务中，特征有时候并不总是连续值，有可能是一些分类值，如性别可分为“male”和“female”。在机器学习任务中，对于这样的特征，通常我们需要对其进行特征数字化，如下面的例子：\n",
    "有如下三个特征属性：\n",
    "性别：[\"male\"，\"female\"]\n",
    "地区：[\"Europe\"，\"US\"，\"Asia\"]\n",
    "浏览器：[\"Firefox\"，\"Chrome\"，\"Safari\"，\"Internet Explorer\"]\n",
    "对于某一个样本，如[\"male\"，\"US\"，\"Internet Explorer\"]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。\n",
    "### 2、One-Hot Encoding的处理方法\n",
    "    对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是思维的，这样，我们可以采用One-Hot编码的方式对上述的样本“[\"male\"，\"US\"，\"Internet Explorer\"]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0,0,0,1]。则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、sklearn例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  1.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "array = enc.transform([[0, 1, 3]]).toarray()\n",
    "print array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六) 缺失值的插补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上篇文章我们讲了五种方法来解决缺失值的问题，其实sklearn里也有一个工具Imputer可以对缺失值进行插补。Imputer类可以对缺失值进行：\n",
    "    均值插补、\n",
    "    中位数插补\n",
    "    或者某行/列出现的频率最高的值进行插补，\n",
    "    也可以对不同的缺失值进行编码。\n",
    "    并且支持稀疏矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "# 用均值插补缺失值\n",
    "imp = Imputer(missing_values='NaN',\n",
    "              strategy='mean', axis=0)\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  2.        ],\n",
       "       [ 6.        ,  3.66666667],\n",
       "       [ 7.        ,  6.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对稀疏矩阵进行缺失值插补\n",
    "import scipy.sparse as sp\n",
    "X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    "imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    "imp.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  2.        ],\n",
       "       [ 6.        ,  3.66666667],\n",
       "       [ 7.        ,  6.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    "imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在稀疏矩阵中，缺失值被编码为0存储为矩阵中，这种格式是适合于缺失值比非缺失值多得多的情况。此外，Imputer类也可以用于Pipeline中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 举个实例(在用随机森林算法之前先用Imputer类进行处理)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Root\\0-soft\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "n_samples, n_features = X_full.shape[0], X_full.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506L, 13L)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with the entire dataset = 0.56\n"
     ]
    }
   ],
   "source": [
    "# Estimate the score on the entire dataset, with no missing values\n",
    "estimator = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "score = cross_val_score(estimator, X_full, y_full).mean()\n",
    "print \"Score with the entire dataset = %.2f\" % score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-6e8240ffe0e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_missing_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmissing_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n\u001b[1;32m----> 5\u001b[1;33m                                       dtype=np.bool),\n\u001b[0m\u001b[0;32m      6\u001b[0m                              np.ones(n_missing_samples,\n\u001b[0;32m      7\u001b[0m                                      dtype=np.bool)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an index"
     ]
    }
   ],
   "source": [
    "# Add missing values in 75% of the lines\n",
    "missing_rate = 0.75\n",
    "n_missing_samples = np.floor(n_samples * missing_rate)\n",
    "missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n",
    "                                      dtype=np.bool),\n",
    "                             np.ones(n_missing_samples,\n",
    "                                     dtype=np.bool)))\n",
    "#    有问题先把代码 码这吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_smples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-7bfbfb9eb6bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_smples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmissing_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_missing_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Estimate the score without the lines containing missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmissing_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'missing_smples' is not defined"
     ]
    }
   ],
   "source": [
    "rng.shuffle(missing_smples)\n",
    "missing_features = rng.randint(0, n_features, n_missing_samples)\n",
    "\n",
    "# Estimate the score without the lines containing missing values\n",
    "X_filtered = X_full[~missing_samples, :]\n",
    "y_filtered = y_full[~missing_samples]\n",
    "estimator = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(estimator, X_filtered, y_filtered).mean()\n",
    "print \"Score without the samples containing missing values = %.2f\" % score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-8c70fdda03bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Estimate the score after imputation of missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_missing\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m estimator = Pipeline([('imputer', Imputer(missing_values=0,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'missing_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# Estimate the score after imputation of missing values\n",
    "X_missing = X_full.copy()\n",
    "X_missing[np.where(missing_samples)[0], missing_features] = 0\n",
    "y_missing = y_full.copy()\n",
    "estimator = Pipeline([('imputer', Imputer(missing_values=0,\n",
    "                                         strategy='mean',\n",
    "                                         axis=0)),\n",
    "                     (\"forest\", RandomForestRegressor(random_state = 0,\n",
    "                                                      n_estimators=100))])\n",
    "score = cross_val_score(estimator, X_missing, y_missing).mean()\n",
    "print \"Score after imputation of the missing values = %.2f\" % score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七）生成多项式特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在输入数据中增加非线性特征可以有效的提高模型的复杂度。\n",
    "### 简单且常用的方法就是使用多项式特征（polynomial features),可以得到特征的高阶交叉项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   3.,   4.,   6.,   9.],\n",
       "       [  1.,   4.,   5.,  16.,  20.,  25.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这样， 就被转化成 . \n",
    "\n",
    "### 然而有时候我们只需要特征的交叉项，可以设置interaction_only=True来得到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9).reshape(3, 3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n",
       "       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n",
       "       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 八) 自定义转换\n",
    "### 如果以上的方法觉得都不够，譬如你想用对数据取对数，可以自己用 FunctionTransformer自定义一个转化器,并且可以在Pipeline中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.69314718],\n",
       "       [ 1.09861229,  1.38629436]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p) #括号内的就是自定义的函数\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 告诉你怎么用：\n",
    "\n",
    "### 如果你在做一个分类任务时，发现第一主成分与这个不相关，你可以用FunctionTransformer把第一列除去，剩下的列用PCA："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntqnKFwInMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa4bf358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa69b828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def _generate_vector(shift=0.5, noise=15):\n",
    "    return np.arange(1000) + (np.random.rand(1000) - shift) * noise\n",
    "\n",
    "def generate_dataset():\n",
    "    \"\"\"\n",
    "    This dataset is two lines with a slope ~ 1, where one has\n",
    "    a y offset of ~100\n",
    "    \"\"\"\n",
    "    return np.vstack((\n",
    "        np.vstack((\n",
    "            _generate_vector(),\n",
    "            _generate_vector() + 100,\n",
    "        )).T,\n",
    "        np.vstack((\n",
    "            _generate_vector(),\n",
    "            _generate_vector(),\n",
    "        )).T,\n",
    "    )), np.hstack((np.zeros(1000), np.ones(1000)))\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "def drop_first_component(X, y):\n",
    "    \"\"\"\n",
    "    Create a pipeline with PCA and the column selector and use it to\n",
    "    transform the dataset.\n",
    "    \"\"\"\n",
    "    pipeline = make_pipeline(\n",
    "        PCA(), FunctionTransformer(all_but_first_column),\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline.transform(X_test), y_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = generate_dataset() #X.shape:(2000L, 2L), y.shape: (2000L,)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "    plt.show()\n",
    "    X_transformed, y_transformed = drop_first_component(*generate_dataset())\n",
    "    plt.scatter(X_transformed[:, 0],\n",
    "                np.zeros(len(X_transformed)),\n",
    "                c = y_transformed,\n",
    "                s=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.13674289e+00,   1.05626935e+00,   8.45209315e+00,\n",
       "         9.19559546e+00,   1.00894984e+01,  -2.04697992e+00,\n",
       "         9.72202330e-01,   1.29798570e+01,   2.54631458e+00,\n",
       "         1.28589427e+01,   6.23790409e+00,   4.51599230e+00,\n",
       "         1.29901868e+01,   1.55486022e+01,   1.34104859e+01,\n",
       "         1.69058133e+01,   1.57969717e+01,   1.12433533e+01,\n",
       "         2.38180944e+01,   1.62998330e+01,   1.34886184e+01,\n",
       "         2.41287908e+01,   2.94324807e+01,   2.88598979e+01,\n",
       "         2.64005407e+01,   2.37962368e+01,   2.90315276e+01,\n",
       "         2.20996635e+01,   2.40721535e+01,   2.77399921e+01,\n",
       "         3.67294377e+01,   3.69338622e+01,   3.86424393e+01,\n",
       "         3.07256439e+01,   2.70793533e+01,   3.45766232e+01,\n",
       "         3.92655777e+01,   4.27484632e+01,   3.85072456e+01,\n",
       "         3.58541673e+01,   4.66103165e+01,   3.75718705e+01,\n",
       "         4.44440314e+01,   3.66085619e+01,   5.11093067e+01,\n",
       "         3.89430969e+01,   5.24549123e+01,   5.26892412e+01,\n",
       "         4.16579703e+01,   5.21023244e+01,   4.39535529e+01,\n",
       "         4.87970563e+01,   4.77004555e+01,   5.83026151e+01,\n",
       "         5.39851799e+01,   6.01088118e+01,   6.31788057e+01,\n",
       "         5.84952719e+01,   6.18681035e+01,   5.94584052e+01,\n",
       "         5.79717183e+01,   5.91238299e+01,   6.39296045e+01,\n",
       "         5.55432252e+01,   6.47146405e+01,   6.26568339e+01,\n",
       "         6.61416539e+01,   5.98358531e+01,   6.45356345e+01,\n",
       "         6.41100069e+01,   6.89620784e+01,   7.33629814e+01,\n",
       "         6.68870240e+01,   6.93697672e+01,   8.07323799e+01,\n",
       "         8.22462038e+01,   6.96726496e+01,   7.56748505e+01,\n",
       "         7.17058343e+01,   8.43555222e+01,   8.55783814e+01,\n",
       "         8.51995866e+01,   7.48638604e+01,   8.76735397e+01,\n",
       "         8.54099149e+01,   8.53008342e+01,   8.41753905e+01,\n",
       "         8.85450284e+01,   9.19162281e+01,   8.72663200e+01,\n",
       "         9.09367489e+01,   9.29018651e+01,   9.50003587e+01,\n",
       "         8.92178363e+01,   9.56069561e+01,   9.84071463e+01,\n",
       "         9.05484138e+01,   9.67752701e+01,   9.79726363e+01,\n",
       "         9.61874178e+01,   9.78104647e+01,   9.82128500e+01,\n",
       "         1.06228149e+02,   1.08496513e+02,   1.00222216e+02,\n",
       "         1.08105210e+02,   1.05810451e+02,   1.11781437e+02,\n",
       "         1.14368482e+02,   1.04373473e+02,   1.04648258e+02,\n",
       "         1.14042153e+02,   1.14209524e+02,   1.19358563e+02,\n",
       "         1.14644245e+02,   1.11317000e+02,   1.15571515e+02,\n",
       "         1.19623452e+02,   1.13099078e+02,   1.23736872e+02,\n",
       "         1.22771253e+02,   1.15698718e+02,   1.28861668e+02,\n",
       "         1.28461479e+02,   1.20817613e+02,   1.18066941e+02,\n",
       "         1.31546802e+02,   1.34266177e+02,   1.31089557e+02,\n",
       "         1.22874461e+02,   1.33481086e+02,   1.35820792e+02,\n",
       "         1.28692947e+02,   1.35295786e+02,   1.38273495e+02,\n",
       "         1.30941009e+02,   1.32283823e+02,   1.40790777e+02,\n",
       "         1.34274459e+02,   1.43450127e+02,   1.39230355e+02,\n",
       "         1.41901992e+02,   1.40421612e+02,   1.42327963e+02,\n",
       "         1.43415845e+02,   1.48782054e+02,   1.42196724e+02,\n",
       "         1.54245265e+02,   1.52332990e+02,   1.50142978e+02,\n",
       "         1.45999849e+02,   1.46821789e+02,   1.54821099e+02,\n",
       "         1.60041253e+02,   1.49445674e+02,   1.51499269e+02,\n",
       "         1.53562435e+02,   1.62949002e+02,   1.60984729e+02,\n",
       "         1.62036646e+02,   1.61776746e+02,   1.57110188e+02,\n",
       "         1.62925238e+02,   1.56967326e+02,   1.70424611e+02,\n",
       "         1.61186807e+02,   1.64999669e+02,   1.68930646e+02,\n",
       "         1.66656133e+02,   1.70698337e+02,   1.77216587e+02,\n",
       "         1.75156570e+02,   1.73144902e+02,   1.74863161e+02,\n",
       "         1.74826422e+02,   1.68767425e+02,   1.73481261e+02,\n",
       "         1.77760565e+02,   1.78586897e+02,   1.80458662e+02,\n",
       "         1.85849631e+02,   1.78330376e+02,   1.85300096e+02,\n",
       "         1.85271354e+02,   1.81921682e+02,   1.88163543e+02,\n",
       "         1.86523379e+02,   1.86255598e+02,   1.86412139e+02,\n",
       "         1.91511156e+02,   1.89845330e+02,   1.97082510e+02,\n",
       "         1.87603858e+02,   1.93005778e+02,   1.95248906e+02,\n",
       "         1.88963323e+02,   2.02017674e+02,   1.91095470e+02,\n",
       "         1.97591191e+02,   1.94912056e+02,   1.99287893e+02,\n",
       "         1.93814702e+02,   1.96663024e+02,   2.09797742e+02,\n",
       "         2.07665575e+02,   2.08887929e+02,   2.04597926e+02,\n",
       "         2.12600244e+02,   2.03781431e+02,   2.15981153e+02,\n",
       "         2.02754073e+02,   2.10546655e+02,   2.06609606e+02,\n",
       "         2.16523557e+02,   2.15560270e+02,   2.10851735e+02,\n",
       "         2.09786522e+02,   2.21442969e+02,   2.11012417e+02,\n",
       "         2.12749113e+02,   2.19067278e+02,   2.23424921e+02,\n",
       "         2.17962051e+02,   2.28425516e+02,   2.19673559e+02,\n",
       "         2.21036519e+02,   2.30607672e+02,   2.25695532e+02,\n",
       "         2.31078170e+02,   2.35528039e+02,   2.29773380e+02,\n",
       "         2.30985694e+02,   2.36612625e+02,   2.26057568e+02,\n",
       "         2.40005798e+02,   2.31488664e+02,   2.30830233e+02,\n",
       "         2.37699436e+02,   2.31044780e+02,   2.43704396e+02,\n",
       "         2.43292669e+02,   2.43242282e+02,   2.43224855e+02,\n",
       "         2.43553987e+02,   2.38474447e+02,   2.41813960e+02,\n",
       "         2.45523715e+02,   2.45404283e+02,   2.48133808e+02,\n",
       "         2.42957664e+02,   2.44001474e+02,   2.55576707e+02,\n",
       "         2.45368536e+02,   2.58448469e+02,   2.51634099e+02,\n",
       "         2.49719696e+02,   2.61749179e+02,   2.54206261e+02,\n",
       "         2.61857160e+02,   2.58526246e+02,   2.67059604e+02,\n",
       "         2.64379472e+02,   2.67134381e+02,   2.55942272e+02,\n",
       "         2.59785752e+02,   2.70244084e+02,   2.73082151e+02,\n",
       "         2.72541366e+02,   2.67892695e+02,   2.75066702e+02,\n",
       "         2.64212265e+02,   2.77481674e+02,   2.78215009e+02,\n",
       "         2.70433351e+02,   2.73309074e+02,   2.72386407e+02,\n",
       "         2.77177640e+02,   2.84218464e+02,   2.80259350e+02,\n",
       "         2.81216660e+02,   2.74073345e+02,   2.77272562e+02,\n",
       "         2.87577667e+02,   2.88003017e+02,   2.86741067e+02,\n",
       "         2.92156266e+02,   2.91811696e+02,   2.89032838e+02,\n",
       "         2.80545671e+02,   2.95357646e+02,   2.93092235e+02,\n",
       "         2.97704243e+02,   2.94157971e+02,   2.89310093e+02,\n",
       "         2.90124222e+02,   2.99146655e+02,   2.98156655e+02,\n",
       "         2.96896324e+02,   2.97393149e+02,   2.92339968e+02,\n",
       "         2.94990267e+02,   2.96003937e+02,   2.96714082e+02,\n",
       "         2.97734854e+02,   3.07141491e+02,   3.04279747e+02,\n",
       "         3.03914582e+02,   3.12199035e+02,   3.02773071e+02,\n",
       "         3.10324058e+02,   3.14573594e+02,   3.16610561e+02,\n",
       "         3.06403393e+02,   3.20221295e+02,   3.14354183e+02,\n",
       "         3.11221243e+02,   3.19439646e+02,   3.23387142e+02,\n",
       "         3.19299878e+02,   3.13813019e+02,   3.17616032e+02,\n",
       "         3.17886053e+02,   3.20100861e+02,   3.26832225e+02,\n",
       "         3.24150775e+02,   3.23565046e+02,   3.29066141e+02,\n",
       "         3.28133968e+02,   3.20819195e+02,   3.32445556e+02,\n",
       "         3.31195309e+02,   3.29700804e+02,   3.29281653e+02,\n",
       "         3.31077979e+02,   3.27767791e+02,   3.36621353e+02,\n",
       "         3.41309960e+02,   3.37602720e+02,   3.36519789e+02,\n",
       "         3.39073931e+02,   3.41307033e+02,   3.46614890e+02,\n",
       "         3.48816774e+02,   3.41731403e+02,   3.36899139e+02,\n",
       "         3.50641615e+02,   3.39686153e+02,   3.50932239e+02,\n",
       "         3.45189868e+02,   3.53091021e+02,   3.43599854e+02,\n",
       "         3.46771599e+02,   3.56466063e+02,   3.55635552e+02,\n",
       "         3.55621092e+02,   3.50015832e+02,   3.62062073e+02,\n",
       "         3.56136474e+02,   3.55618436e+02,   3.51548642e+02,\n",
       "         3.66413928e+02,   3.66805925e+02,   3.61042620e+02,\n",
       "         3.69540035e+02,   3.60097064e+02,   3.60694330e+02,\n",
       "         3.67600374e+02,   3.63367810e+02,   3.64721778e+02,\n",
       "         3.74457706e+02,   3.73653228e+02,   3.67180583e+02,\n",
       "         3.71023580e+02,   3.66472365e+02,   3.70189906e+02,\n",
       "         3.72150563e+02,   3.70476270e+02,   3.76541558e+02,\n",
       "         3.72855653e+02,   3.82707937e+02,   3.74277556e+02,\n",
       "         3.81824833e+02,   3.85822828e+02,   3.77886502e+02,\n",
       "         3.91051197e+02,   3.81118696e+02,   3.84322832e+02,\n",
       "         3.81775589e+02,   3.91580252e+02,   3.88346013e+02,\n",
       "         3.92645626e+02,   3.83869773e+02,   3.86416041e+02,\n",
       "         3.88515656e+02,   3.92902624e+02,   3.90076148e+02,\n",
       "         4.01077277e+02,   3.93028354e+02,   4.04542374e+02,\n",
       "         3.98377140e+02,   3.98326271e+02,   3.95156433e+02,\n",
       "         4.04208254e+02,   3.96078903e+02,   4.01392523e+02,\n",
       "         4.03506614e+02,   4.12025976e+02,   4.05497739e+02,\n",
       "         4.02264677e+02,   4.08285889e+02,   4.05675981e+02,\n",
       "         4.18489611e+02,   4.15554766e+02,   4.18175367e+02,\n",
       "         4.19452012e+02,   4.13867683e+02,   4.22186292e+02,\n",
       "         4.22954077e+02,   4.14522069e+02,   4.25970284e+02,\n",
       "         4.22777723e+02,   4.24783919e+02,   4.25252802e+02,\n",
       "         4.22737637e+02,   4.30826597e+02,   4.29701858e+02,\n",
       "         4.30069916e+02,   4.24276525e+02,   4.20990266e+02,\n",
       "         4.26973949e+02,   4.24275728e+02,   4.34965346e+02,\n",
       "         4.31933459e+02,   4.33924041e+02,   4.27961781e+02,\n",
       "         4.30547257e+02,   4.39957342e+02,   4.39063739e+02,\n",
       "         4.40799291e+02,   4.32797287e+02,   4.40446214e+02,\n",
       "         4.37912093e+02,   4.41902884e+02,   4.41992122e+02,\n",
       "         4.41211545e+02,   4.44995210e+02,   4.41675732e+02,\n",
       "         4.53533818e+02,   4.53476163e+02,   4.41997252e+02,\n",
       "         4.49166579e+02,   4.55652259e+02,   4.47119348e+02,\n",
       "         4.59640246e+02,   4.60105079e+02,   4.54510037e+02,\n",
       "         4.52945055e+02,   4.54989996e+02,   4.61038622e+02,\n",
       "         4.63987177e+02,   4.57745434e+02,   4.54691443e+02,\n",
       "         4.63587367e+02,   4.68771063e+02,   4.58141194e+02,\n",
       "         4.71998057e+02,   4.67289519e+02,   4.66148060e+02,\n",
       "         4.65954261e+02,   4.61974500e+02,   4.70443771e+02,\n",
       "         4.65988517e+02,   4.71679174e+02,   4.69414756e+02,\n",
       "         4.75203003e+02,   4.75438808e+02,   4.80454132e+02,\n",
       "         4.81962012e+02,   4.77908596e+02,   4.76011234e+02,\n",
       "         4.80132497e+02,   4.79514451e+02,   4.89490336e+02,\n",
       "         4.79269153e+02,   4.89982891e+02,   4.80538688e+02,\n",
       "         4.83919749e+02,   4.83900362e+02,   4.95064607e+02,\n",
       "         4.95909310e+02,   4.94419904e+02,   4.96760328e+02,\n",
       "         4.84564867e+02,   4.98749945e+02,   4.86784724e+02,\n",
       "         4.98937234e+02,   4.89723310e+02,   4.89901088e+02,\n",
       "         4.91682906e+02,   4.92681115e+02,   5.01405470e+02,\n",
       "         5.05515238e+02,   5.08967577e+02,   5.02502523e+02,\n",
       "         5.08068199e+02,   5.09095254e+02,   4.99067562e+02,\n",
       "         5.09253257e+02,   5.14021458e+02,   5.10154323e+02,\n",
       "         5.10034064e+02,   5.11936303e+02,   5.10148525e+02,\n",
       "         5.15416958e+02,   5.12948602e+02,   5.15314736e+02,\n",
       "         5.21306183e+02,   5.10554844e+02,   5.13515262e+02,\n",
       "         5.24167627e+02,   5.14526607e+02,   5.20508047e+02,\n",
       "         5.22731713e+02,   5.15813608e+02,   5.25423814e+02,\n",
       "         5.19896834e+02,   5.32271936e+02,   5.23687998e+02,\n",
       "         5.33257863e+02,   5.21934394e+02,   5.23939049e+02,\n",
       "         5.27767420e+02,   5.25518750e+02,   5.36203825e+02,\n",
       "         5.34466265e+02,   5.36440355e+02,   5.39408992e+02,\n",
       "         5.43810662e+02,   5.34263017e+02,   5.40548057e+02,\n",
       "         5.43964029e+02,   5.46369124e+02,   5.34635827e+02,\n",
       "         5.50400210e+02,   5.46512935e+02,   5.40478305e+02,\n",
       "         5.49768863e+02,   5.51833642e+02,   5.51603488e+02,\n",
       "         5.42260494e+02,   5.51968890e+02,   5.45024127e+02,\n",
       "         5.51297539e+02,   5.46331215e+02,   5.59408904e+02,\n",
       "         5.49247724e+02,   5.58538117e+02,   5.51893245e+02,\n",
       "         5.50583115e+02,   5.66113174e+02,   5.53456823e+02,\n",
       "         5.55088420e+02,   5.67239193e+02,   5.66709790e+02,\n",
       "         5.64537403e+02,   5.62249186e+02,   5.71598581e+02,\n",
       "         5.61361587e+02,   5.69166608e+02,   5.65993592e+02,\n",
       "         5.72884552e+02,   5.70540350e+02,   5.65525722e+02,\n",
       "         5.78866264e+02,   5.76820636e+02,   5.79063179e+02,\n",
       "         5.78051229e+02,   5.81272513e+02,   5.71910363e+02,\n",
       "         5.75988450e+02,   5.85536840e+02,   5.85536210e+02,\n",
       "         5.87653787e+02,   5.79230856e+02,   5.79595743e+02,\n",
       "         5.85633488e+02,   5.83261825e+02,   5.93155407e+02,\n",
       "         5.91489920e+02,   5.96476758e+02,   5.96612340e+02,\n",
       "         5.87298201e+02,   5.85717147e+02,   5.92629627e+02,\n",
       "         6.00609829e+02,   5.90305228e+02,   5.91745466e+02,\n",
       "         5.98721353e+02,   5.99994659e+02,   5.93943743e+02,\n",
       "         5.95578053e+02,   6.03873952e+02,   6.05033781e+02,\n",
       "         6.04015737e+02,   5.99365801e+02,   6.03208448e+02,\n",
       "         6.13444239e+02,   6.11369987e+02,   6.07276673e+02,\n",
       "         6.02078543e+02,   6.09422593e+02,   6.10068773e+02,\n",
       "         6.12787838e+02,   6.18243337e+02,   6.12767515e+02,\n",
       "         6.20204431e+02,   6.10217104e+02,   6.16559483e+02,\n",
       "         6.24170655e+02,   6.13825376e+02,   6.21253907e+02,\n",
       "         6.18009115e+02,   6.19831222e+02,   6.21699364e+02,\n",
       "         6.20746197e+02,   6.23454993e+02,   6.26563216e+02,\n",
       "         6.29420466e+02,   6.29361020e+02,   6.26451669e+02,\n",
       "         6.31798666e+02,   6.35655829e+02,   6.32198568e+02,\n",
       "         6.28998011e+02,   6.36253004e+02,   6.34260304e+02,\n",
       "         6.40671820e+02,   6.33455832e+02,   6.36853082e+02,\n",
       "         6.44963319e+02,   6.34125177e+02,   6.47216960e+02,\n",
       "         6.44739494e+02,   6.35846856e+02,   6.39379854e+02,\n",
       "         6.47779491e+02,   6.43023761e+02,   6.46377643e+02,\n",
       "         6.45446932e+02,   6.48076506e+02,   6.45075869e+02,\n",
       "         6.45630276e+02,   6.49106056e+02,   6.58804088e+02,\n",
       "         6.56905051e+02,   6.59492488e+02,   6.51132379e+02,\n",
       "         6.60599770e+02,   6.62730031e+02,   6.55307651e+02,\n",
       "         6.64276469e+02,   6.64870661e+02,   6.64473692e+02,\n",
       "         6.66023780e+02,   6.63463137e+02,   6.61566579e+02,\n",
       "         6.66802315e+02,   6.61056907e+02,   6.73901987e+02,\n",
       "         6.71043053e+02,   6.71312827e+02,   6.70567021e+02,\n",
       "         6.68812769e+02,   6.77279392e+02,   6.71579426e+02,\n",
       "         6.69875239e+02,   6.71274344e+02,   6.73458834e+02,\n",
       "         6.81814198e+02,   6.73119204e+02,   6.73972268e+02,\n",
       "         6.81939608e+02,   6.84329510e+02,   6.82109054e+02,\n",
       "         6.85160361e+02,   6.90135428e+02,   6.82586178e+02,\n",
       "         6.87683962e+02,   6.82229220e+02,   6.92681719e+02,\n",
       "         6.86926254e+02,   6.95083401e+02,   6.89310308e+02,\n",
       "         7.00340058e+02,   6.98841993e+02,   6.88430405e+02,\n",
       "         6.90632857e+02,   6.95428135e+02,   6.99182932e+02,\n",
       "         6.98324197e+02,   7.01766314e+02,   7.08447975e+02,\n",
       "         6.96415827e+02,   7.03196147e+02,   7.02701659e+02,\n",
       "         6.99588901e+02,   7.01800473e+02,   7.08299633e+02,\n",
       "         7.02197873e+02,   7.09684074e+02,   7.08953935e+02,\n",
       "         7.04211710e+02,   7.11444723e+02,   7.16070160e+02,\n",
       "         7.15352135e+02,   7.12301515e+02,   7.16595082e+02,\n",
       "         7.11805896e+02,   7.22329093e+02,   7.17804340e+02,\n",
       "         7.15370490e+02,   7.19600199e+02,   7.18401412e+02,\n",
       "         7.16703348e+02,   7.17581253e+02,   7.18064861e+02,\n",
       "         7.23815762e+02,   7.30247760e+02,   7.24767595e+02,\n",
       "         7.25358443e+02,   7.30680030e+02,   7.37265182e+02,\n",
       "         7.28038183e+02,   7.33519666e+02,   7.32433879e+02,\n",
       "         7.40435395e+02,   7.36904814e+02,   7.40670851e+02,\n",
       "         7.37474902e+02,   7.43390075e+02,   7.42609029e+02,\n",
       "         7.37827980e+02,   7.35951471e+02,   7.40497644e+02,\n",
       "         7.47450290e+02,   7.47377578e+02,   7.46722081e+02,\n",
       "         7.51172157e+02,   7.49265031e+02,   7.47267046e+02,\n",
       "         7.53164711e+02,   7.45523570e+02,   7.58125355e+02,\n",
       "         7.56160117e+02,   7.47854168e+02,   7.61638709e+02,\n",
       "         7.62566622e+02,   7.50087331e+02,   7.59655893e+02,\n",
       "         7.56750513e+02,   7.52996289e+02,   7.66318815e+02,\n",
       "         7.61748311e+02,   7.68279103e+02,   7.65409703e+02,\n",
       "         7.57939681e+02,   7.61847060e+02,   7.68008615e+02,\n",
       "         7.72636980e+02,   7.68202971e+02,   7.69581074e+02,\n",
       "         7.65920442e+02,   7.67553227e+02,   7.74995538e+02,\n",
       "         7.77145248e+02,   7.73548358e+02,   7.72017196e+02,\n",
       "         7.82998826e+02,   7.72883691e+02,   7.75507995e+02,\n",
       "         7.74155161e+02,   7.79643977e+02,   7.75241663e+02,\n",
       "         7.84813266e+02,   7.77905892e+02,   7.78437895e+02,\n",
       "         7.91430686e+02,   7.82259699e+02,   7.95224632e+02,\n",
       "         7.95995190e+02,   7.95443043e+02,   7.96664839e+02,\n",
       "         7.84617705e+02,   7.92945253e+02,   7.95707370e+02,\n",
       "         8.00152398e+02,   8.02552577e+02,   7.99548880e+02,\n",
       "         8.02639376e+02,   7.99960004e+02,   7.92650492e+02,\n",
       "         8.00080791e+02,   7.95395400e+02,   8.08216023e+02,\n",
       "         7.99929175e+02,   8.03975853e+02,   8.03214041e+02,\n",
       "         8.00954895e+02,   8.12566974e+02,   8.03942344e+02,\n",
       "         8.09704569e+02,   8.12588156e+02,   8.08250850e+02,\n",
       "         8.08560132e+02,   8.08436087e+02,   8.10934196e+02,\n",
       "         8.21462149e+02,   8.15989729e+02,   8.22565719e+02,\n",
       "         8.18520466e+02,   8.18838993e+02,   8.27933101e+02,\n",
       "         8.19630532e+02,   8.19635898e+02,   8.29646918e+02,\n",
       "         8.27396510e+02,   8.23632007e+02,   8.32072730e+02,\n",
       "         8.24507296e+02,   8.34259774e+02,   8.27158505e+02,\n",
       "         8.38355044e+02,   8.34050297e+02,   8.29090855e+02,\n",
       "         8.40285917e+02,   8.34570253e+02,   8.41988561e+02,\n",
       "         8.32868625e+02,   8.39116115e+02,   8.37744382e+02,\n",
       "         8.45664845e+02,   8.43721958e+02,   8.42796113e+02,\n",
       "         8.42481948e+02,   8.39333592e+02,   8.44162606e+02,\n",
       "         8.45945215e+02,   8.54166272e+02,   8.42114260e+02,\n",
       "         8.52860330e+02,   8.48201167e+02,   8.56660339e+02,\n",
       "         8.51075490e+02,   8.59648688e+02,   8.54347307e+02,\n",
       "         8.58399820e+02,   8.49631015e+02,   8.57174942e+02,\n",
       "         8.52942900e+02,   8.53768162e+02,   8.61158031e+02,\n",
       "         8.58457798e+02,   8.59822197e+02,   8.60117624e+02,\n",
       "         8.68904728e+02,   8.67431853e+02,   8.67025726e+02,\n",
       "         8.71409956e+02,   8.62458963e+02,   8.74076332e+02,\n",
       "         8.71431510e+02,   8.69338765e+02,   8.77209980e+02,\n",
       "         8.71679605e+02,   8.75379906e+02,   8.80018446e+02,\n",
       "         8.70904465e+02,   8.76837122e+02,   8.72080045e+02,\n",
       "         8.76175663e+02,   8.73606446e+02,   8.85287951e+02,\n",
       "         8.82220966e+02,   8.90042589e+02,   8.79351887e+02,\n",
       "         8.89623423e+02,   8.93100031e+02,   8.88404595e+02,\n",
       "         8.91623601e+02,   8.92045082e+02,   8.92690617e+02,\n",
       "         8.89472702e+02,   8.87727197e+02,   8.97719502e+02,\n",
       "         8.94610633e+02,   8.99155362e+02,   9.00120782e+02,\n",
       "         8.95306032e+02,   8.91055797e+02,   8.97360603e+02,\n",
       "         8.95329607e+02,   9.00932603e+02,   9.01602195e+02,\n",
       "         9.01914539e+02,   9.09276918e+02,   9.10746645e+02,\n",
       "         9.11560559e+02,   9.07533286e+02,   9.06590218e+02,\n",
       "         9.16289411e+02,   9.12042117e+02,   9.15116866e+02,\n",
       "         9.16912520e+02,   9.10555657e+02,   9.12418130e+02,\n",
       "         9.21667212e+02,   9.14605754e+02,   9.20500341e+02,\n",
       "         9.15841955e+02,   9.22382665e+02,   9.18537509e+02,\n",
       "         9.19616447e+02,   9.28330267e+02,   9.27817153e+02,\n",
       "         9.19434212e+02,   9.28299586e+02,   9.30192704e+02,\n",
       "         9.29591794e+02,   9.32336085e+02,   9.24345864e+02,\n",
       "         9.35961113e+02,   9.36590843e+02,   9.34206364e+02,\n",
       "         9.35962214e+02,   9.31721412e+02,   9.40102856e+02,\n",
       "         9.35880389e+02,   9.37378192e+02,   9.44303682e+02,\n",
       "         9.36456544e+02,   9.34504008e+02,   9.46414392e+02,\n",
       "         9.38304331e+02,   9.35761753e+02,   9.40026137e+02,\n",
       "         9.38698555e+02,   9.49069749e+02,   9.47669329e+02,\n",
       "         9.54859747e+02,   9.43831579e+02,   9.57064816e+02,\n",
       "         9.55195366e+02,   9.44561798e+02,   9.46151611e+02,\n",
       "         9.57676761e+02,   9.52170592e+02,   9.52645106e+02,\n",
       "         9.52408435e+02,   9.63124575e+02,   9.60783070e+02,\n",
       "         9.59734111e+02,   9.55576868e+02,   9.63322326e+02,\n",
       "         9.64538054e+02,   9.63833414e+02,   9.60333888e+02,\n",
       "         9.67858825e+02,   9.67958935e+02,   9.60922365e+02,\n",
       "         9.62852013e+02,   9.76067750e+02,   9.67585463e+02,\n",
       "         9.72234976e+02,   9.70837751e+02,   9.77658866e+02,\n",
       "         9.77549276e+02,   9.70418238e+02,   9.81755747e+02,\n",
       "         9.71837173e+02,   9.72819448e+02,   9.80355292e+02,\n",
       "         9.87492905e+02,   9.79432333e+02,   9.85633772e+02,\n",
       "         9.86787660e+02,   9.84917032e+02,   9.92961914e+02,\n",
       "         9.84181310e+02,   9.83562892e+02,   9.92666616e+02,\n",
       "         9.92804818e+02,   9.96029451e+02,   9.95469984e+02,\n",
       "         9.97215696e+02,   9.98024100e+02,   9.89683256e+02,\n",
       "         9.97710454e+02,   9.94244257e+02,   1.00483745e+03,\n",
       "         1.00116904e+03])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = _generate_vector()\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000L, 2L), (2000L,))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, Y1 = generate_dataset()\n",
    "X1.shape, Y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((np.zeros(10), np.ones(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
